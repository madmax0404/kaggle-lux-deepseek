{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1877cb44-5429-4a5a-b709-c5b5737bd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(8)\n",
    "\n",
    "# ✅ Enable FlashAttention for faster inference\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# ✅ Ensure pad token is set correctly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ Optimized quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True  # ✅ Add nested quantization for better memory usage\n",
    ")\n",
    "\n",
    "# ✅ Load model with proper device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # Let Accelerate handle device placement\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d50d20-2fcc-4db3-8166-6eb055d81284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460c5aac-d16a-4500-8d87-9149cf8935b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, I need to create a new command in the markdown file, but I don't know how to do it. I have tried searching for some commands, but nothing seems to work. Could you help me with this?\n",
      "\n",
      "I have a markdown file with some markdown syntax, but I also have some code snippets. I need to add a new command with a certain name, but I don't know the syntax to do that. I tried looking at some markdown documentation, but nothing seems to help. I need to figure out how to create a new command in the markdown file.\n",
      "\n",
      "I have tried searching for \"how to create a new command in markdown\" or \"how to define a new command in markdown,\" but the results are confusing. Maybe the user is trying to add a custom command in the markdown file, but the syntax isn't clear. How can I create a new command in the markdown file, similar to how it's done in Python or JavaScript?\n",
      "\n",
      "I have a basic understanding of Markdown syntax, so I can focus on how to define a new command in the markdown file, perhaps using syntax similar to Python or JavaScript. Maybe I can define it using an arrow or function syntax, but I'm not sure. I have tried searching for \"how to create a new command in markdown\" and found some links, but they are not helpful for me. Maybe I should just try writing it out.\n",
      "\n",
      "I have tried writing a simple command using an arrow, like this:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$MyCommand$$\n",
      "```\n",
      "\n",
      "But that doesn't seem to work. Maybe I need to use some special syntax, like $$ or \\. Or perhaps I need to use a function or class syntax. I'm not sure how to proceed. I think I need to look for a way to define a new command in the markdown file, maybe using a function or a class, similar to how it's done in Python or JavaScript.\n",
      "\n",
      "Wait, I remember that in Python, you can define a function, and in JavaScript, you can define a function or use a class. Maybe in Markdown, I can define a function-like syntax to create a new command. So, perhaps I can use an arrow with some parameters and return a value, similar to a function.\n",
      "\n",
      "So, maybe I can define a new command using an arrow, like this:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$MyCommand$$\n",
      "$$f(MyCommand, arg)$$\n",
      "```\n",
      "\n",
      "But I'm not sure if that will work. Maybe I need to use a different syntax. Alternatively, perhaps I can define a new function or class in the markdown file, similar to how it's done in JavaScript.\n",
      "\n",
      "Wait, in JavaScript, you can define a function using the function syntax, like:\n",
      "\n",
      "```javascript\n",
      "function MyCommand(arg) {\n",
      "  // code\n",
      "}\n",
      "```\n",
      "\n",
      "So, maybe in Markdown, I can define a function-like syntax, but it's not straightforward. I'm not sure if that's possible.\n",
      "\n",
      "Alternatively, perhaps I can use a syntax similar to the `class` keyword in Python, but I don't think that's supported in Markdown. So, maybe I can't define a new class in the markdown file.\n",
      "\n",
      "Wait, I think in Markdown, you can define a new command by using a syntax similar to how it's done in Python. Maybe I can define it using an arrow with parameters and a return value.\n",
      "\n",
      "So, for example, I can write:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$f(arg)$$\n",
      "$$f(arg) = result$$\n",
      "```\n",
      "\n",
      "But I'm not sure if that's valid. Maybe I need to use some other syntax, like $$f$$ and $$result$$.\n",
      "\n",
      "Alternatively, perhaps I can define a new command using an arrow with parameters and a return value, similar to how it's done in Python.\n",
      "\n",
      "So, maybe I can write:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$f(arg)$$\n",
      "$$f(arg) = result$$\n",
      "```\n",
      "\n",
      "But I'm not sure if that will work. Maybe I need to use some other syntax, like $$f(arg) = result$$.\n",
      "\n",
      "Alternatively, perhaps I can use a function-like syntax, but I'm not sure how.\n",
      "\n",
      "Wait, I think in Markdown, you can't directly define functions or classes. So, perhaps the best way is to use an arrow with parameters and return a value.\n",
      "\n",
      "So, maybe I can write:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$f(arg)$$\n",
      "$$f(arg) = result$$\n",
      "```\n",
      "\n",
      "But I'm not sure if that will work. Maybe I need to use some other syntax, like $$f(arg)$$ and $$result$$.\n",
      "\n",
      "Alternatively, perhaps I can define a new command using an arrow with parameters and return a value, like:\n",
      "\n",
      "```markdown\n",
      "# MyCommand\n",
      "$$f(arg)$$\n",
      "$$f(arg) = result$$\n",
      "```\n",
      "\n",
      "But I'm not sure if that will produce the desired output.\n",
      "\n",
      "Wait, maybe I can use the `class` keyword in Markdown, but I don't think that's supported. So\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 1024\n",
    "\n",
    "# ✅ Optimized input preparation\n",
    "prompt = \"hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ✅ Optimized generation parameters\n",
    "generated_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    use_cache=True  # ✅ Enable KV caching for faster generation\n",
    ")\n",
    "\n",
    "# ✅ Efficient decoding\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19586766-d435-4779-989f-0283d8aff2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_new_tokens = 1024\n",
    "\n",
    "# ✅ Optimized input preparation\n",
    "prompt = \"can you tell what these mean? sensor_mask: '[[False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]\\n [False False False False False False False False False False False False False False False False False False False False False False False False]]', map_features_energy: '[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\\n [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ✅ Optimized generation parameters\n",
    "generated_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    use_cache=True  # ✅ Enable KV caching for faster generation\n",
    ")\n",
    "\n",
    "# ✅ Efficient decoding\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068f129-818b-4091-b500-3e7b72bfc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024\n",
    "\n",
    "# ✅ Optimized input preparation\n",
    "prompt = \"what the fuck is going on bitch ass\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e03ec-8602-400a-b94b-b39cb8f82bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea883a4-2d64-4f02-89a0-aa01deeeea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eddb26-27d4-457d-b415-9e8473a0e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c763bb-51d0-4241-81a6-c35337fcc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cc788-ce92-43e7-b7c3-e5d238fa6265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a08f6-81f3-459e-8337-1af5eb0c6cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed7f0b-2be2-4e37-b842-1923b6935137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34db086-71d1-44bc-82e3-14074984a8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d944a8-6c36-46eb-b790-613061b102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad9362-e231-48d1-87ed-7af279400967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Optimized generation parameters\n",
    "generated_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    use_cache=True  # ✅ Enable KV caching for faster generation\n",
    ")\n",
    "\n",
    "# ✅ Efficient decoding\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be242f5a-3e7b-45df-a926-a442aba6f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7846a-2d5d-4574-b742-58509ae74279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1ecc7-dc91-46bd-95eb-b895304b8ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddbbd1-6e5b-4dac-93e4-3ec45ae04760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fa800-4c12-4822-ae4c-8db709ae074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cb935-62ae-4ac4-b5ec-1966082b8284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ca722-d63e-4f64-bb86-690d1f6f54d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b3b74-60c8-4015-b285-7e659971e521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae532c-d220-4127-9290-b20dc7bcf5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c5535-3f48-451e-b8a2-1b748cd832d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243054f3-9ab6-4dc6-a628-bde4910958b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918f5c7-e3e7-4e2c-b86e-ccc01b87d150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d0102-9972-4dc9-8716-4f6638d3ad50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427647e-df15-456f-855d-9aaaa6828c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f676c-5f16-4965-9657-46b4fbded45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7a8e-4cb2-40b9-afc4-de639e7059eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184b058-3d84-4379-95e5-18ab8e8e4c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bd5a9-b78a-4eda-bdeb-59a724dddafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f68a7d-459a-48b5-9e93-ac0103f4d936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13303f-22ee-4b4c-851e-75714cbf555c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961bb77-64ae-47ed-bfae-b6f87440e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024\n",
    "\n",
    "# ✅ Optimized input preparation\n",
    "prompt = 12351235123\n",
    "#inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ✅ Optimized generation parameters\n",
    "generated_output = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    use_cache=True  # ✅ Enable KV caching for faster generation\n",
    ")\n",
    "\n",
    "# ✅ Efficient decoding\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d8cfb-1cca-46cf-b94d-f22c33cb88be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018c3f-084e-48d6-a882-c3aac39876dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95461ca-5ab9-44d6-8d63-b02a0fab089e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec87edc-3f53-4739-b123-7bca805c01f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be4ff5-b0f6-416f-87da-98a850adba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bfd23-1311-44de-a814-d5e9c8ffedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20baf732-d7fc-46ec-9d1d-a6867d376cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3306e-f954-41f4-84fe-4f9aeada433b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51eef1-6d52-481d-8105-3217f6a5cad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1942043-4076-4950-900b-7021692eb8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ✅ Enable FlashAttention for faster inference (if using PyTorch 2.0+)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# ✅ Ensure pad token is set correctly\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ✅ Use BF16 instead of FP16 (More stable, similar performance)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # ✅ 4-bit quantization (Faster than 8-bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # ✅ Use BF16 instead of FP16\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized 4-bit (best performance)\n",
    ")\n",
    "\n",
    "# ✅ Load model with quantization & auto device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # ✅ Use BF16 (better stability)\n",
    "    device_map=\"auto\",  # Automatically allocate layers across GPU & CPU\n",
    "    quantization_config=bnb_config,  # Use BitsAndBytesConfig for quantization\n",
    ").to('cuda')\n",
    "\n",
    "# ✅ Compile model for faster execution (PyTorch 2.0+)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e049-5c0b-49ef-90c0-efa060ae4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 4096  # Number of tokens to generate\n",
    "\n",
    "# ✅ Prepare input and move it to GPU\n",
    "prompt = \"hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# ✅ Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# ✅ Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed2322-c412-436d-a777-35cd82b3e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load model with quantization & auto device placement\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # ✅ Use BF16 (better stability)\n",
    "    device_map=\"auto\",  # Automatically allocate layers across GPU & CPU\n",
    "    quantization_config=bnb_config,  # Use BitsAndBytesConfig for quantization\n",
    ").to('cuda')\n",
    "\n",
    "# ✅ Compile model for faster execution (PyTorch 2.0+)\n",
    "model2 = torch.compile(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4588df5-0c02-4edb-8f58-ea3611a07e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10395677,
     "sourceId": 86411,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.048125,
   "end_time": "2024-12-26T20:02:43.573801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T20:00:20.525676",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
