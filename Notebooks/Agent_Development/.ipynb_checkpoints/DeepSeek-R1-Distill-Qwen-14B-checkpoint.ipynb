{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac428fe-5754-49f3-9326-5400f1a0f057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37d5da-18fe-4f78-b62f-3d39fb8b3888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289ce64-beb5-40e1-b34b-3b347c5c5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ✅ Enable FlashAttention for faster inference (if using PyTorch 2.0+)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "\n",
    "# ✅ Use BF16 instead of FP16 (More stable, similar performance)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # ✅ 4-bit quantization (Faster than 8-bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # ✅ Use BF16 instead of FP16\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized 4-bit (best performance)\n",
    ")\n",
    "\n",
    "# ✅ Load model with quantization & auto device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # ✅ Use BF16 (better stability)\n",
    "    device_map=\"auto\",  # Automatically allocate layers across GPU & CPU\n",
    "    quantization_config=bnb_config,  # Use BitsAndBytesConfig for quantization\n",
    ")\n",
    "\n",
    "# ✅ Compile model for faster execution (PyTorch 2.0+)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1735b3-c672-4ef7-89ad-d2f92bdc73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024  # Number of tokens to generate\n",
    "\n",
    "# ✅ Ensure pad token is set correctly\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ✅ Prepare input and move it to GPU\n",
    "prompt = \"hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# ✅ Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# ✅ Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510ba8f-64a7-4cd1-8f57-a685f333f663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed78ee-88b8-4d0c-91b9-1661161228b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ff6b8-53ef-45e6-a1e2-4da9488842ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a618c21-d57f-454e-8bfd-05c71d82a77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1afc99-a878-4ab9-87c4-443809254051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53d5f8-2d36-413d-982e-cf1d56b6c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "\n",
    "# Define quantization config for 8-bit loading\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True  # 8-bit quantization to save GPU memory\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # Automatically use available GPU\n",
    "    quantization_config=bnb_config  # Use new BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015115a-c995-44c1-8d7c-d565264c4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e400580-037a-4c00-aed5-a92f09dbbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e86a97-ad4c-4156-b79b-e45826c31ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure pad token is set correctly\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare input with attention mask\n",
    "prompt = \"hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"]  # Explicitly pass attention_mask\n",
    "\n",
    "# Generate output\n",
    "generated_output = model.generate(**inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=max_new_tokens)\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f928e-1951-4bff-b9e1-4578a43bbe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10984da9-fe03-443a-95a0-032144243bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c4e09-a4ca-4381-b3ce-4a285a17b15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af9e8c-030f-4494-880c-d47ffca577a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19db8e-1b53-45e3-bdfe-a7be533746d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input with attention mask\n",
    "prompt = \"sex\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"]  # Explicitly pass attention_mask\n",
    "\n",
    "# Generate output\n",
    "generated_output = model.generate(**inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=max_new_tokens)\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e5d5f-1aee-4df7-bf38-9fe475767a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input with attention mask\n",
    "prompt = \"sex me\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"]  # Explicitly pass attention_mask\n",
    "\n",
    "# Generate output\n",
    "generated_output = model.generate(**inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=max_new_tokens)\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d24897-50c5-4806-8917-978d8579c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input with attention mask\n",
    "prompt = \"sex you\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"]  # Explicitly pass attention_mask\n",
    "\n",
    "# Generate output\n",
    "generated_output = model.generate(**inputs, pad_token_id=tokenizer.pad_token_id, max_new_tokens=max_new_tokens)\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50886c-0e1c-48ee-8c3a-a09a606a8e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63893185-3a41-4d84-bf18-aba9b5cc9d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c4ad0-9ed5-42f0-bb2a-23461421be43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e0c45-721f-4944-9bd3-6b40df389789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009bacd-efa2-4ce2-b40d-88c5651ac32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4a5b2-5924-411f-b7f3-44cbb62570d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1b083-b554-4e6a-9c5f-62bc450ce219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc684a02-16b3-466e-9ec4-4be5dac97699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4896e-e1a5-4bea-9537-4d0a6d3f2687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e94654-991a-4036-90c2-01813d2247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073abd0-11ce-4dac-9a37-6583d94906c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843b4de-b722-475f-8b47-4c8580652712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcd14d-ef66-43be-9967-825c3714c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Language Generation: Use the transformer model for a status report.\n",
    "            prompt = f\"Unit {idx} assigned task {task} with counter strategy {counter_strategy}.\"\n",
    "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            generated_output = self.model.generate(input_ids, max_length=20)\n",
    "            language_response = self.tokenizer.decode(generated_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4158d-55de-4a2b-a1ff-03fb578817a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae54ce-323a-419a-ae3b-bee8dcc3b668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c30082-4139-4330-83d8-0387db284e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bd567-366e-4df9-bc8d-4b0cc4af7433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198ab4b-79d0-4ede-ad06-ee81c87e936b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "480ec4e4-ff2b-46f1-be67-46ef8d8c4629",
   "metadata": {},
   "source": [
    "# My Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656aa61-f0c6-4c79-9d00-d87647fcc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Test/agent.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import heapq\n",
    "from lux.utils import direction_to\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# ------------------------\n",
    "# Neural network components\n",
    "# ------------------------\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(WorldModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "    def fine_tune(self, state, target_weights, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_weights = self.forward(state)\n",
    "        loss = nn.MSELoss()(predicted_weights, target_weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# ------------------------\n",
    "# Policy and evaluation modules\n",
    "# ------------------------\n",
    "class SelfImprovingPolicy:\n",
    "    def __init__(self, learning_rate=0.0003):\n",
    "        # Create a PPO agent using a simple gym environment for demonstration.\n",
    "        self.env = gym.make(\"LuxAI-v3\")\n",
    "        self.model = PPO(\"MlpPolicy\", self.env, verbose=1)\n",
    "        self.optimizer = optim.Adam(self.model.policy.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def update_policy(self, state, reward, action):\n",
    "        self.optimizer.zero_grad()\n",
    "        # For demonstration purposes, we assume evaluate_actions returns a tuple\n",
    "        loss = -reward * self.model.policy.evaluate_actions(state, action)[0].mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def select_action(self, observation):\n",
    "        # The PPO predict returns a tuple (action, _)\n",
    "        return self.model.predict(observation, deterministic=False)\n",
    "\n",
    "class RiskRewardEvaluator:\n",
    "    def __init__(self, risk_tolerance=0.5, aggression_factor=1.2, efficiency_factor=0.8):\n",
    "        self.risk_tolerance = risk_tolerance\n",
    "        self.aggression_factor = aggression_factor\n",
    "        self.efficiency_factor = efficiency_factor\n",
    "    \n",
    "    def evaluate(self, reward, uncertainty, energy_efficiency, aggression_level):\n",
    "        adjusted_reward = reward - (self.risk_tolerance * uncertainty)\n",
    "        adjusted_reward += (self.aggression_factor * aggression_level) - (self.efficiency_factor * (1 - energy_efficiency))\n",
    "        return adjusted_reward\n",
    "\n",
    "class ExplorationExploitationBalancer:\n",
    "    def __init__(self, exploration_weight=0.85):\n",
    "        self.exploration_weight = exploration_weight\n",
    "    \n",
    "    def balance(self, unit_state, known_rewards):\n",
    "        exploration_factor = np.random.uniform(0, 1)\n",
    "        if exploration_factor < self.exploration_weight:\n",
    "            return np.random.choice(range(len(known_rewards)))  # Explore\n",
    "        return np.argmax(known_rewards)  # Exploit\n",
    "\n",
    "# ------------------------\n",
    "# Prediction and planning modules\n",
    "# ------------------------\n",
    "class TemporalOpponentPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TemporalOpponentPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "class AIPathPlanner:\n",
    "    def __init__(self, grid_size=(24, 24)):\n",
    "        self.grid_size = grid_size\n",
    "    \n",
    "    def heuristic(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "    \n",
    "    def a_star_search(self, start, goal, obstacles):\n",
    "        open_set = []\n",
    "        heapq.heappush(open_set, (0, start))\n",
    "        came_from = {}\n",
    "        cost_so_far = {start: 0}\n",
    "        \n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "            if current == goal:\n",
    "                break\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                neighbor = (current[0] + dx, current[1] + dy)\n",
    "                if (0 <= neighbor[0] < self.grid_size[0] and \n",
    "                    0 <= neighbor[1] < self.grid_size[1] and \n",
    "                    neighbor not in obstacles):\n",
    "                    new_cost = cost_so_far[current] + 1\n",
    "                    if neighbor not in cost_so_far or new_cost < cost_so_far[neighbor]:\n",
    "                        cost_so_far[neighbor] = new_cost\n",
    "                        priority = new_cost + self.heuristic(goal, neighbor)\n",
    "                        heapq.heappush(open_set, (priority, neighbor))\n",
    "                        came_from[neighbor] = current\n",
    "        \n",
    "        path = []\n",
    "        current = goal\n",
    "        while current in came_from:\n",
    "            path.append(current)\n",
    "            current = came_from[current]\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "# ------------------------\n",
    "# Opponent modeling and adaptive strategy\n",
    "# ------------------------\n",
    "class OpponentMemory:\n",
    "    def __init__(self, max_size=300):\n",
    "        self.max_size = max_size\n",
    "        self.memory = []\n",
    "    \n",
    "    def add(self, data):\n",
    "        if len(self.memory) >= self.max_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(data)\n",
    "    \n",
    "    def get_recent(self, num=15):\n",
    "        return self.memory[-num:]\n",
    "    \n",
    "    def analyze_opponent(self):\n",
    "        if len(self.memory) < 15:\n",
    "            return \"unknown\"\n",
    "        move_patterns = [entry['move'] for entry in self.memory[-15:]]\n",
    "        aggression_levels = [entry['aggression'] for entry in self.memory[-15:]]\n",
    "        \n",
    "        avg_aggression = np.mean(aggression_levels)\n",
    "        if avg_aggression > 0.8:\n",
    "            return \"hyper-aggressive\"\n",
    "        elif avg_aggression > 0.6:\n",
    "            return \"aggressive\"\n",
    "        elif avg_aggression < 0.3:\n",
    "            return \"defensive\"\n",
    "        return \"balanced\"\n",
    "\n",
    "class AdaptiveOpponentStrategy:\n",
    "    def __init__(self):\n",
    "        self.counter_strategies = {\n",
    "            \"hyper-aggressive\": \"trap\",\n",
    "            \"aggressive\": \"defensive\",\n",
    "            \"defensive\": \"resource-hogging\",\n",
    "            \"balanced\": \"opportunistic\",\n",
    "            \"unknown\": \"cautious\"\n",
    "        }\n",
    "    \n",
    "    def get_counter_strategy(self, opponent_style):\n",
    "        return self.counter_strategies.get(opponent_style, \"cautious\")\n",
    "\n",
    "class EnergyOptimizer:\n",
    "    def __init__(self, efficiency_factor=0.93):\n",
    "        self.efficiency_factor = efficiency_factor\n",
    "    \n",
    "    def optimize(self, energy_available, energy_needed):\n",
    "        return min(energy_available, energy_needed * self.efficiency_factor)\n",
    "\n",
    "class HRLController:\n",
    "    def __init__(self):\n",
    "        self.high_level_tasks = [\"scout\", \"harvest\", \"combat\", \"defend\", \"ambush\", \"trap\"]\n",
    "    \n",
    "    def assign_task(self, unit_state, opponent_style):\n",
    "        if opponent_style == \"hyper-aggressive\" and unit_state['energy'] > 80:\n",
    "            return \"trap\"\n",
    "        elif opponent_style == \"aggressive\" and unit_state['energy'] > 70:\n",
    "            return \"defend\"\n",
    "        elif unit_state['energy'] > 100:\n",
    "            return \"combat\"\n",
    "        elif unit_state['energy'] > 60:\n",
    "            return \"harvest\"\n",
    "        elif opponent_style == \"defensive\":\n",
    "            return \"ambush\"\n",
    "        return \"scout\"\n",
    "\n",
    "class CommunicationModule:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def broadcast(self, message):\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def receive(self):\n",
    "        return self.messages.pop(0) if self.messages else None\n",
    "\n",
    "class AdaptiveCombatStrategy:\n",
    "    def __init__(self):\n",
    "        self.aggressiveness_threshold = 0.85\n",
    "    \n",
    "    def adjust_combat(self, opponent_behavior, unit_state):\n",
    "        if opponent_behavior == \"hyper-aggressive\":\n",
    "            return \"set-trap\" if unit_state['energy'] > 90 else \"evade\"\n",
    "        elif opponent_behavior == \"aggressive\":\n",
    "            return \"evade\" if unit_state['energy'] < 50 else \"counterattack\"\n",
    "        elif opponent_behavior == \"defensive\":\n",
    "            return \"press\" if unit_state['energy'] > 75 else \"hold\"\n",
    "        return \"balanced\"\n",
    "\n",
    "# ------------------------\n",
    "# Agent Class: Integrating all modules\n",
    "# ------------------------\n",
    "class Agent():\n",
    "    def __init__(self, player: str, env_cfg) -> None:\n",
    "        self.player = player\n",
    "        self.enemy_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.enemy_team_id = 1 if self.team_id == 0 else 0\n",
    "        self.env_cfg = env_cfg\n",
    "        self.unit_move_cost = env_cfg['unit_move_cost']\n",
    "        self.unit_sap_cost = env_cfg['unit_sap_cost']\n",
    "        self.unit_sap_range = env_cfg['unit_sap_range']\n",
    "        self.unit_sensor_range = env_cfg['unit_sensor_range']\n",
    "        self.map_height = env_cfg['map_height']\n",
    "        self.map_width = env_cfg['map_width']\n",
    "        self.first_spawn = False\n",
    "        \n",
    "        self.map_explored_status = np.zeros((self.map_height, self.map_width), dtype=int)\n",
    "        \n",
    "        # Modules used in decision making:\n",
    "        self.opponent_memory = OpponentMemory(max_size=100)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "        self.world_model = WorldModel(input_dim=20, hidden_dim=128, output_dim=20)\n",
    "        self.meta_learner = MetaLearner(input_dim=10, hidden_dim=64, output_dim=3)\n",
    "        self.self_improving_policy = SelfImprovingPolicy()\n",
    "        self.risk_reward_evaluator = RiskRewardEvaluator()\n",
    "        self.exploration_exploitation_balancer = ExplorationExploitationBalancer()\n",
    "        self.temporal_predictor = TemporalOpponentPredictor(input_dim=5, hidden_dim=64, output_dim=5)\n",
    "        self.ai_path_planner = AIPathPlanner(grid_size=(self.map_height, self.map_width))\n",
    "        self.adaptive_opponent_strategy = AdaptiveOpponentStrategy()\n",
    "        self.energy_optimizer = EnergyOptimizer()\n",
    "        self.hrl_controller = HRLController()\n",
    "        self.communication_module = CommunicationModule()\n",
    "        self.adaptive_combat_strategy = AdaptiveCombatStrategy()\n",
    "    \n",
    "    def act(self, step: int, obs, remainingOverageTime: int = 60):\n",
    "        # ------------------------\n",
    "        # 1. Opponent Modeling and Temporal Prediction\n",
    "        # ------------------------\n",
    "        # (Simulate adding an opponent move to memory)\n",
    "        self.opponent_memory.add({'move': np.random.rand(), 'aggression': np.random.rand()})\n",
    "        opponent_style = self.opponent_memory.analyze_opponent()\n",
    "        \n",
    "        # Prepare a sequence for the temporal predictor.\n",
    "        recent_memory = self.opponent_memory.get_recent(5)\n",
    "        # Extract 'move' values; pad if needed.\n",
    "        moves = [entry.get('move', 0) for entry in recent_memory]\n",
    "        if len(moves) < 5:\n",
    "            moves += [0] * (5 - len(moves))\n",
    "        # TemporalOpponentPredictor expects input shape (batch, sequence, feature_dim)\n",
    "        # Here, we repeat each move value 5 times to form a dummy 5-dimensional vector.\n",
    "        state_sequence = torch.tensor([[[m] * 5 for m in moves]], dtype=torch.float32)\n",
    "        predicted_opponent_move = self.temporal_predictor(state_sequence).argmax().item()\n",
    "        \n",
    "        unit_actions = {}\n",
    "        \n",
    "        # ------------------------\n",
    "        # 2. Process Each Unit’s Decision\n",
    "        # ------------------------\n",
    "        for idx, pos in enumerate(obs['units']['position'][self.team_id]):\n",
    "            # Simulate unit state (here, energy is randomly set for demonstration)\n",
    "            unit_energy = np.random.randint(50, 150)\n",
    "            unit_state = {'energy': unit_energy, 'position': pos}\n",
    "            \n",
    "            # High-Level Task Assignment (via HRL Controller)\n",
    "            task = self.hrl_controller.assign_task(unit_state, opponent_style)\n",
    "            \n",
    "            # Path Planning: For tasks that involve movement, plan a path.\n",
    "            if task in [\"harvest\", \"scout\", \"combat\"]:\n",
    "                # For demonstration, set the goal as the center of the map.\n",
    "                goal = (self.map_height // 2, self.map_width // 2)\n",
    "                obstacles = []  # In a real scenario, populate this with known obstacles.\n",
    "                path = self.ai_path_planner.a_star_search(tuple(pos), goal, obstacles)\n",
    "            else:\n",
    "                path = []\n",
    "            \n",
    "            # Adaptive Combat: If the task is combat, adjust combat strategy.\n",
    "            if task == \"combat\":\n",
    "                combat_strategy = self.adaptive_combat_strategy.adjust_combat(opponent_style, unit_state)\n",
    "            else:\n",
    "                combat_strategy = None\n",
    "            \n",
    "            # Adaptive Opponent Strategy: Determine a counter strategy.\n",
    "            counter_strategy = self.adaptive_opponent_strategy.get_counter_strategy(opponent_style)\n",
    "            \n",
    "            # Energy Optimization: Decide how much energy to allocate.\n",
    "            energy_needed = np.random.randint(30, 100)\n",
    "            optimized_energy = self.energy_optimizer.optimize(unit_energy, energy_needed)\n",
    "            \n",
    "            # Action Selection: Use the self-improving policy to choose an action.\n",
    "            action, _ = self.self_improving_policy.select_action(obs)\n",
    "            \n",
    "            # Risk-Reward Evaluation: Evaluate the situation.\n",
    "            team_points = obs['team_points'][self.team_id]\n",
    "            enemy_points = obs['team_points'][self.enemy_team_id]\n",
    "            reward_diff = team_points - enemy_points\n",
    "            risk_adjusted_reward = self.risk_reward_evaluator.evaluate(\n",
    "                reward_diff,\n",
    "                np.random.rand(),                 # uncertainty (simulated)\n",
    "                np.random.uniform(0.7, 1.0),        # energy efficiency (simulated)\n",
    "                np.random.rand()                  # aggression level (simulated)\n",
    "            )\n",
    "            \n",
    "            # Update the policy based on the risk-adjusted reward.\n",
    "            self.self_improving_policy.update_policy(obs, risk_adjusted_reward, action)\n",
    "            \n",
    "            # Meta Learning: Fine-tune meta parameters.\n",
    "            dummy_state = torch.rand(10)\n",
    "            target_weights = torch.rand(3)\n",
    "            meta_optimizer = optim.Adam(self.meta_learner.parameters(), lr=0.001)\n",
    "            meta_loss = self.meta_learner.fine_tune(dummy_state, target_weights, meta_optimizer)\n",
    "            \n",
    "            # World Model: Predict future state based on a dummy input.\n",
    "            dummy_input = torch.rand(20)\n",
    "            predicted_state = self.world_model(dummy_input)\n",
    "            \n",
    "            # Exploration vs Exploitation: Choose a mode based on known rewards.\n",
    "            known_rewards = [risk_adjusted_reward, np.random.rand(), np.random.rand()]\n",
    "            exploration_choice = self.exploration_exploitation_balancer.balance(unit_state, known_rewards)\n",
    "            \n",
    "            # Language Generation: Use the transformer model for a status report.\n",
    "            prompt = f\"Unit {idx} assigned task {task} with counter strategy {counter_strategy}.\"\n",
    "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            generated_output = self.model.generate(input_ids, max_length=20)\n",
    "            language_response = self.tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Communication: Broadcast the decision.\n",
    "            message = (\n",
    "                f\"Unit {idx}: Task {task}, Path: {path}, Combat: {combat_strategy}, \"\n",
    "                f\"Optimized Energy: {optimized_energy}, Exploration Choice: {exploration_choice}, \"\n",
    "                f\"Language Response: {language_response}\"\n",
    "            )\n",
    "            self.communication_module.broadcast(message)\n",
    "            \n",
    "            # Combine all decisions into the final unit action.\n",
    "            unit_actions[idx] = {\n",
    "                'action': action,\n",
    "                'task': task,\n",
    "                'path': path,\n",
    "                'combat_strategy': combat_strategy,\n",
    "                'optimized_energy': optimized_energy,\n",
    "                'exploration_choice': exploration_choice,\n",
    "                'language_response': language_response,\n",
    "                'predicted_opponent_move': predicted_opponent_move,\n",
    "                'meta_loss': meta_loss,\n",
    "                'predicted_state': predicted_state.detach().numpy()\n",
    "            }\n",
    "        \n",
    "        # Optionally, process any incoming communications.\n",
    "        comms = []\n",
    "        received = self.communication_module.receive()\n",
    "        while received is not None:\n",
    "            comms.append(received)\n",
    "            received = self.communication_module.receive()\n",
    "        # (Here you could log or otherwise process the communications)\n",
    "        \n",
    "        return unit_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce9aa1-e2db-4430-8bdc-2cf853669ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810404d3-e978-4db0-9705-5e3ddc8096b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473e953-3803-4daf-a70d-036980127d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a3753-7adb-415a-8d0e-6a99e34d0e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb1df4-6dda-4f10-9c75-7386933b2237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f3aa5-b7e5-476d-a08b-c097949eaeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5a512-98e1-4db2-857e-91ac30563ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87d4eb-72a6-468e-aea0-9a4bbfd7f4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e9adc-aba3-470a-9ce5-80d77b9f4103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1ff10-655b-473f-9d08-a168ce14febd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1d53d-0d72-4b71-bca4-283a6869ac34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e2aec-4b6b-40a2-8bea-442d959314e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f17b1-6a0e-4817-9168-5c0af2b43f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469c73b-d8e7-4da0-9b24-9e8732c477e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc24e2b-b55a-47b1-967f-339a51d0e3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961a908-c196-4fab-9643-84c6fdd13c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c226967-5cdc-4d19-bf57-9b931d1f8f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f134d0-8a7e-4999-aad7-73ab991ce4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31ec9a-6bb3-4b58-91ac-0d273c8625ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d3f35-28ea-4304-8861-29d56a966603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9eb9a-ed96-4e92-80b6-5f6757e6aba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580a5d2-90bc-4d9a-8ae1-0e58528a6948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1516ac-f690-4db5-9137-00f546503fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287b74d-48b5-4eeb-981b-206db12bd484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fef54-2e61-4cf6-bfbb-2b3c8a6cfdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d372f2-23c3-4f7c-9d9d-dc1a3ef60710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3497d-9155-4ed8-a394-53c298dddd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6451666-5aab-4ea7-b70e-8cdd98f3106c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64f012-0bac-46fc-b795-e8876dd0705b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad66f7-cb73-49c2-9b5c-6bc8d71a123b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31b2ab-ed28-46cc-b5d8-c6670a1a3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!luxai-s3 Test/main.py Test/main.py --output=replay_my_agent.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e049-5c0b-49ef-90c0-efa060ae4fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10395677,
     "sourceId": 86411,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.048125,
   "end_time": "2024-12-26T20:02:43.573801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T20:00:20.525676",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
