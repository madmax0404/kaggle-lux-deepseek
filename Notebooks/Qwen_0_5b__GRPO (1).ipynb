{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First we install vllm. Notice that you'll have to restart the session afterwards."
      ],
      "metadata": {
        "id": "GOMhew_59RbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "1Haibc22C6WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/Lux-AI-Challenge/Lux-Design-S3\n",
        "# %cd Lux-Design-S3\n",
        "!pip install -e src"
      ],
      "metadata": {
        "id": "p5tezr9VClxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools -q\n",
        "!pip install luxai-s3 transformers vllm"
      ],
      "metadata": {
        "id": "PYykgnUJ0BdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl datasets -q"
      ],
      "metadata": {
        "id": "ybtxR89X1YJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "WZT7fw0xBSIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/Lux-AI-Challenge/Lux-Design-S3\n",
        "!mv Lux-Design-S3 lux_ai_env"
      ],
      "metadata": {
        "id": "y4L7UYIfA-p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self_play_generator.py\n",
        "import os\n",
        "\n",
        "from lux1.src.luxai_s3.env import LuxAIS3Env\n",
        "import json\n",
        "import random\n",
        "\n",
        "def generate_self_play_data(num_episodes=100):\n",
        "    data = []\n",
        "    env = LuxAIS3Env()\n",
        "    env.reset(42)\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        env.reset()\n",
        "        done = False\n",
        "        episode_data = {\n",
        "            \"states\": [],\n",
        "            \"actions\": [],\n",
        "            \"rewards\": []\n",
        "        }\n",
        "\n",
        "        while not done:\n",
        "            state = env.render()\n",
        "            episode_data[\"states\"].append(state)\n",
        "\n",
        "            # Generate random actions for both teams\n",
        "            team0_actions = {}\n",
        "            team1_actions = {}\n",
        "            for team in [\"player_0\", \"player_1\"]:\n",
        "                for unit in env.state.units[team].values():\n",
        "                    move = random.choice([\"center\", \"up\", \"right\", \"down\", \"left\"])\n",
        "                    sap_target = (unit.pos.x, unit.pos.y)\n",
        "\n",
        "                    if team == \"player_0\":\n",
        "                        team0_actions[unit.unit_id] = {\n",
        "                            \"move\": move,\n",
        "                            \"sap\": sap_target\n",
        "                        }\n",
        "                    else:\n",
        "                        team1_actions[unit.unit_id] = {\n",
        "                            \"move\": move,\n",
        "                            \"sap\": sap_target\n",
        "                        }\n",
        "\n",
        "            actions = {\n",
        "                \"player_0\": team0_actions,\n",
        "                \"player_1\": team1_actions\n",
        "            }\n",
        "            episode_data[\"actions\"].append(actions)\n",
        "\n",
        "            # Step the environment\n",
        "            obs, reward, done, info = env.step(actions)\n",
        "\n",
        "            episode_data[\"rewards\"].append(reward)\n",
        "\n",
        "        data.append(episode_data)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Save dataset\n",
        "data = generate_self_play_data(100)\n",
        "with open(\"self_play_dataset.json\", \"w\") as f:\n",
        "    json.dump(data, f)"
      ],
      "metadata": {
        "id": "q04kVVaQ6dSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import the gsm8k dataset and restructure it to fit into a conversational prompt format:"
      ],
      "metadata": {
        "id": "38AVgA19-PMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reward_functions.py\n",
        "import json\n",
        "\n",
        "def compute_rewards(state, actions):\n",
        "    reward = 0.0\n",
        "\n",
        "    # Relic collection\n",
        "    relic_collected = state[\"relics_collected\"][\"player_0\"] - state[\"relics_collected\"][\"player_1\"]\n",
        "    reward += relic_collected * 2.0\n",
        "\n",
        "    # Energy preservation\n",
        "    energy0 = sum(u[\"energy\"] for u in state[\"units\"][\"player_0\"].values())\n",
        "    energy1 = sum(u[\"energy\"] for u in state[\"units\"][\"player_1\"].values())\n",
        "    reward += (energy0 - energy1) * 0.1\n",
        "\n",
        "    # Sapping effectiveness\n",
        "    sapped_units = 0\n",
        "    for action in actions[\"player_0\"].values():\n",
        "        if \"sap\" in action:\n",
        "            sapped_units += 1\n",
        "    reward += sapped_units * 0.5\n",
        "\n",
        "    # Collision advantage\n",
        "    if state[\"collision_outcome\"] == \"player_0\":\n",
        "        reward += 1.0\n",
        "    elif state[\"collision_outcome\"] == \"player_1\":\n",
        "        reward -= 1.0\n",
        "\n",
        "    # Terminal match reward\n",
        "    if state[\"real_env_steps\"] >= 100:\n",
        "        if state[\"teams\"][\"player_0\"].score > state[\"teams\"][\"player_1\"].score:\n",
        "            reward += 10.0\n",
        "        else:\n",
        "            reward -= 10.0\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "fno7X8Fh-N6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_grpo.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import json\n",
        "\n",
        "# Load dataset\n",
        "with open(\"self_play_dataset.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Prepare training data\n",
        "grpo_dataset = []\n",
        "for episode in dataset:\n",
        "    for state, action, reward in zip(episode[\"states\"], episode[\"actions\"], episode[\"rewards\"]):\n",
        "        grpo_dataset.append({\n",
        "            \"prompt\": json.dumps(state),\n",
        "            \"completion\": json.dumps(action[\"player_0\"]),\n",
        "            \"reward\": compute_rewards(state, action)\n",
        "        })\n",
        "\n",
        "# Training configuration\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"outputs/lux_ai_agent\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_prompt_length=512,\n",
        "    max_completion_length=256,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\").to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "\n",
        "# Define reward function\n",
        "def reward_func(prompt, completions):\n",
        "    state = json.loads(prompt.split(\"<state>\")[1].split(\"</state>\")[0])\n",
        "    action = json.loads(completions[0][\"content\"])\n",
        "    return compute_rewards(state, action)\n",
        "\n",
        "# Train the model\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=grpo_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "BLCIyOzI0Gol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now set the training arguments:"
      ],
      "metadata": {
        "id": "cuz-LQOQ-vSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# agent.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "\n",
        "class LuxAILLM:\n",
        "    def __init__(self, model_type=\"cpu\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"outputs/lux_ai_agent\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"outputs/lux_ai_agent\").to(\"cuda\" if model_type == \"cuda\" else \"cpu\")\n",
        "\n",
        "    def generate_actions(self, game_state):\n",
        "        prompt = \"\"\"\n",
        "        <rules>\n",
        "        You are playing Lux AI Season 3. Units can move in 5 directions, sap enemy units, and collect relics. Collisions destroy units. Maintain energy and maximize relic collection.\n",
        "        </rules>\n",
        "        <state>\n",
        "        {game_state}\n",
        "        </state>\n",
        "        <think>\n",
        "        Planning optimal moves...\n",
        "        </think>\n",
        "        <action>\n",
        "        {{}}\n",
        "        </action>\n",
        "        \"\"\".format(game_state=game_state)\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=512, temperature=0.7)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        action_start = response.find(\"<action>\") + len(\"<action>\")\n",
        "        action_end = response.find(\"</action>\")\n",
        "        return json.loads(response[action_start:action_end])\n",
        "\n",
        "# Example usage\n",
        "agent = LuxAILLM()\n",
        "game_state = {\n",
        "    \"units\": {\n",
        "        \"player_0\": [{\"id\": \"u0\", \"pos\": (3,4), \"energy\": 80}],\n",
        "        \"player_1\": [{\"id\": \"u1\", \"pos\": (8,9), \"energy\": 65}]\n",
        "    },\n",
        "    \"relics\": [{\"pos\": (5,6)}],\n",
        "    \"energy_nodes\": [{\"pos\": (12,10)}]\n",
        "}\n",
        "actions = agent.generate_actions(game_state)\n",
        "print(actions)"
      ],
      "metadata": {
        "id": "gGFFu5u4-3uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And launch the actual training:"
      ],
      "metadata": {
        "id": "REuVM0ep-4dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_agent.py\n",
        "from luxai_s3.env import LuxAI_S3\n",
        "from agent import LuxAILLM\n",
        "\n",
        "def evaluate_agent(llm_agent, baseline_agent, episodes=100):\n",
        "    env = LuxAI_S3(env_cfg=\"env.cfg\", seed=42)\n",
        "    wins = 0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = env.render()\n",
        "\n",
        "            # LLM agent actions\n",
        "            llm_actions = llm_agent.generate_actions(state)\n",
        "\n",
        "            # Baseline agent actions (random example)\n",
        "            baseline_actions = {}\n",
        "            for unit in env.state.units[\"player_1\"].values():\n",
        "                move = random.choice([\"center\", \"up\", \"right\", \"down\", \"left\"])\n",
        "                baseline_actions[unit.unit_id] = {\n",
        "                    \"move\": move,\n",
        "                    \"sap\": (unit.pos.x, unit.pos.y)\n",
        "                }\n",
        "\n",
        "            # Step environment\n",
        "            obs, reward, done, info = env.step({\n",
        "                \"player_0\": llm_actions,\n",
        "                \"player_1\": baseline_actions\n",
        "            })\n",
        "\n",
        "        if env.state.teams[\"player_0\"].score > env.state.teams[\"player_1\"].score:\n",
        "            wins += 1\n",
        "\n",
        "    return wins / episodes\n",
        "\n",
        "# Run evaluation\n",
        "llm_agent = LuxAILLM()\n",
        "win_rate = evaluate_agent(llm_agent, None, episodes=10)\n",
        "print(f\"Win rate: {win_rate:.2f}\")"
      ],
      "metadata": {
        "id": "U1ixGbPG0Ni-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}