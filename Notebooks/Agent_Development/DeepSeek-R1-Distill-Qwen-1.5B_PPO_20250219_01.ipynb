{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52958575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from trl import PPOConfig#, PPOTrainer\n",
    "#import luxai_s3\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "#from luxai_s3.params import EnvParams\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "#from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "#from accelerate import infer_auto_device_map\n",
    "import gc\n",
    "#import copy\n",
    "gc.enable()\n",
    "\n",
    "#from stable_baselines3 import PPO\n",
    "#import gymnasium as gym\n",
    "#import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "os.environ[\"FLASH_ATTENTION\"] = \"1\"\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "np.set_printoptions(linewidth=200)\n",
    "# Configure CUDA memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,garbage_collection_threshold:0.8\"\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "os.environ[\"PYTORCH_ATTENTION_USE_MEMORY_EFFICIENT_ATTENTION\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f17ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "#dataset = get_gsm8k_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe737742",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(policy_model_name)\n",
    "\n",
    "# ✅ Ensure pad token is set correctly\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "value_model_name = \"distilbert/distilgpt2\"\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "value_tokenizer = AutoTokenizer.from_pretrained(value_model_name)\n",
    "\n",
    "# ✅ Ensure pad token is set correctly\n",
    "value_tokenizer.pad_token = value_tokenizer.eos_token\n",
    "\n",
    "# ✅ Optimized quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,  # ✅ Add nested quantization for better memory usage\n",
    "    bnb_4bit_quant_storage=\"bfloat16\"  # Enable quantized storage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_autoconfig = AutoConfig.from_pretrained(policy_model_name)\n",
    "policy_autoconfig.max_position_embeddings = 11000\n",
    "policy_autoconfig.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_autoconfig = AutoConfig.from_pretrained(value_model_name)\n",
    "#value_autoconfig.max_position_embeddings = 11000\n",
    "value_autoconfig.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_model():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        policy_model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",  # Let Accelerate handle device placement\n",
    "        #device_map={\"0\": \"14GiB\", \"cpu\": \"64GiB\"},  # Let Accelerate handle device placement\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        config=policy_autoconfig,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        #use_cache=False,  # Disable KV cache during training\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # Enable memory efficient features\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_model():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        value_model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",  # Let Accelerate handle device placement\n",
    "        #device_map={\"0\": \"14GiB\", \"cpu\": \"64GiB\"},  # Let Accelerate handle device placement\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        config=value_autoconfig,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        #use_cache=False,  # Disable KV cache during training\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # Enable memory efficient features\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RecordEpisode(\n",
    "    LuxAIS3GymEnv(numpy_output=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad949a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "def strict_format_reward_func(completion) -> float:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    match = re.match(pattern, completion)\n",
    "\n",
    "    return 0.5 if match else 0.0\n",
    "\n",
    "def soft_format_reward_func(completion) -> float:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<answer>.*?</answer>\"\n",
    "    match = re.match(pattern, completion)\n",
    "\n",
    "    return 0.5 if match else 0.0\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completion) -> float:\n",
    "\n",
    "    return count_xml(completion)\n",
    "\n",
    "def answer_format_reward_func(completion) -> float:\n",
    "    r\"\"\"\n",
    "    Computes a reward based on whether the answer text (extracted from the completion)\n",
    "    follows the required format:\n",
    "    \n",
    "    Expected format (one line per unit, 16 total):\n",
    "      Unit 0: 1\n",
    "      Unit 1: 2\n",
    "      Unit 2: 5, 2, 2\n",
    "      Unit 3: 0\n",
    "      Unit 4: 5, 1, 1\n",
    "      Unit 5: 5, -1, -2\n",
    "      Unit 6: 5, -2, 2\n",
    "      Unit 7: 5, 0, 0\n",
    "      Unit 8: 4\n",
    "      Unit 9: 0\n",
    "      Unit 10: 3\n",
    "      Unit 11: 2\n",
    "      Unit 12: 1\n",
    "      Unit 13: 0\n",
    "      Unit 14: 5, -4, 5\n",
    "      Unit 15: 5, 3, -3\n",
    "\n",
    "    Each line is expected to match:\n",
    "      ^Unit\\s+([0-9]+):\\s+((?:[0-4])|(?:5,\\s*-?\\d+,\\s*-?\\d+))$\n",
    "    \n",
    "    Returns:\n",
    "      A list of scores (floats), one per completion.\n",
    "    \"\"\"\n",
    "    # extract_xml_answer should extract the text between the <answer> tags.\n",
    "    answer = extract_xml_answer(completion)\n",
    "    \n",
    "    # Updated regex pattern:\n",
    "    answer_pattern = re.compile(\n",
    "        r\"^Unit\\s+([0-9]+):\\s+((?:[0-4])|(?:5,\\s*-?\\d+,\\s*-?\\d+))$\"\n",
    "    )\n",
    "\n",
    "    answer_score = 0.0\n",
    "    # Split the answer into lines and remove any extra whitespace\n",
    "    lines = [line.strip() for line in answer.strip().split(\"\\n\") if line.strip()]\n",
    "    # Penalize if we do not have exactly 16 lines (one per unit)\n",
    "    if len(lines) != 16:\n",
    "        answer_score -= 0.2  # adjust penalty as desired\n",
    "    \n",
    "    for line in lines:\n",
    "        match = answer_pattern.match(line)\n",
    "        if match:\n",
    "            # Reward for a valid line\n",
    "            answer_score += 0.5 / 16\n",
    "            unit_number = int(match.group(1))\n",
    "            # Ensure unit numbers are within the valid range (0 to 15)\n",
    "            if unit_number < 0 or unit_number > 15:\n",
    "                answer_score -= 0.1 / 16\n",
    "        else:\n",
    "            # Penalize for any line that doesn't match the required format\n",
    "            answer_score -= 0.1\n",
    "\n",
    "    return answer_score\n",
    "\n",
    "def point_gain_reward_func(reward_score) -> float:\n",
    "\n",
    "    return reward_score\n",
    "\n",
    "def match_won_reward_func(match_won) -> float:\n",
    "\n",
    "    return 100.0 if match_won else 0.0\n",
    "\n",
    "def match_lost_reward_func(match_lost) -> float:\n",
    "\n",
    "    return -10.0 if match_lost else 0.0\n",
    "\n",
    "def game_won_reward_func(game_won) -> float:\n",
    "\n",
    "    return 1000.0 if game_won else 0.0\n",
    "\n",
    "def game_lost_reward_func(game_lost) -> float:\n",
    "\n",
    "    return -100.0 if game_lost else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games_to_train = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"outputs/DeepSeek-R1-Distill-Qwen-1.5B-PPO\"\n",
    "run_name=\"DeepSeek-R1-Distill-Qwen-1.5B-PPO-20250217_01\"\n",
    "\n",
    "training_args = PPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_sample_generations=0,\n",
    "    max_grad_norm=0.1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    log_on_each_node=False,\n",
    "    report_to=\"none\",\n",
    "    num_ppo_epochs=1,\n",
    "    cliprange=0.2,\n",
    "    vf_coef=1.0,\n",
    "    kl_coef=0.01,\n",
    "    prediction_loss_only=True,\n",
    "    gradient_checkpointing=True,\n",
    "    #reward_model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    #use_cpu=True,\n",
    "    max_steps=1,\n",
    "    #eval_steps=1,\n",
    "    #eval_accumulation_steps=8,\n",
    "    #accelerator_config={\"num_processes\": 8},\n",
    "    per_device_train_batch_size=1,\n",
    "    #per_device_eval_batch_size=1,\n",
    "    torch_empty_cache_steps=1,\n",
    "    #torch_compile=True,\n",
    "    #torch_compile_mode=\"default\",\n",
    "    total_episodes=num_games_to_train,\n",
    "    micro_batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    local_batch_size=1,\n",
    "    response_length=230,\n",
    "    ds3_gather_for_generation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.num_mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e75ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.mini_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_1 = create_policy_model()\n",
    "policy_model_2 = create_policy_model()\n",
    "#value_model_1 = create_value_model()\n",
    "#value_model_2 = create_value_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_1.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cfd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedPolicyAndValueModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.base = model\n",
    "        # Value head: a lightweight linear layer that maps hidden_size to a scalar.\n",
    "        self.value_head = nn.Linear(model.config.hidden_size, 1).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # Forward pass through the shared transformer backbone.\n",
    "        outputs = self.base.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,  # we need hidden states for value prediction\n",
    "            **kwargs\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]  # final layer: (batch, seq_len, hidden_size)\n",
    "        # Policy logits: project hidden states to vocabulary size.\n",
    "        logits = self.base.lm_head(hidden_states)\n",
    "        # Value: Use a simple pooling strategy.\n",
    "        # For example, use the hidden state corresponding to the first token (or [CLS]) as a summary.\n",
    "        value = self.value_head(hidden_states)\n",
    "        value = value.squeeze(-1)\n",
    "        # Alternatively, you can average the hidden states across the sequence:\n",
    "        # pooled = hidden_states.mean(dim=1)\n",
    "        # value = self.value_head(pooled)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c10e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = SharedPolicyAndValueModel(policy_model_1)\n",
    "model_2 = SharedPolicyAndValueModel(policy_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dddc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modified_PPO_Trainer.ppo_trainer_20250219_01 import ModifiedPPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModifiedPPOTrainer(\n",
    "    model=policy_model_1,\n",
    "    model_2=policy_model_2,\n",
    "    processing_class=policy_tokenizer,\n",
    "    args=training_args,\n",
    "    reward_functions=[\n",
    "        strict_format_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        xmlcount_reward_func,\n",
    "        answer_format_reward_func,\n",
    "        point_gain_reward_func,\n",
    "        match_won_reward_func,\n",
    "        match_lost_reward_func,\n",
    "        game_won_reward_func,\n",
    "        game_lost_reward_func\n",
    "    ],\n",
    "    game_env=env,\n",
    "    num_games_to_train=num_games_to_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64afff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349959e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d4b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee5af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb71bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1db81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import GradientAccumulationPlugin\n",
    "from accelerate.utils.dataclasses import DeepSpeedPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46983e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plugin = DeepSpeedPlugin(\n",
    "    # hf_ds_config={\n",
    "    #     \"distributed_type\": \"NO\",\n",
    "    # },\n",
    "    gradient_accumulation_steps=8,\n",
    "    is_train_batch_min=True,\n",
    "    #offload_optimizer_device=\"cpu\",\n",
    "    #offload_param_device=\"cpu\",\n",
    "    zero_stage=2,\n",
    ")\n",
    "plugin = GradientAccumulationPlugin(sync_with_dataloader=False, num_steps=8, sync_each_batch=False)\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\", gradient_accumulation_plugin=plugin, deepspeed_plugin=ds_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1080c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.state.deepspeed_plugin.deepspeed_config[\"train_micro_batch_size_per_gpu\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd50a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10395677,
     "sourceId": 86411,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.048125,
   "end_time": "2024-12-26T20:02:43.573801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T20:00:20.525676",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
