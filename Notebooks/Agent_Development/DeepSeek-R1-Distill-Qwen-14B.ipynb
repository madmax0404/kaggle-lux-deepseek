{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260b2d49-08eb-424b-8f39-4d06bc5f9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fad983b96104ec9a4e78b78b5c1421a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# âœ… Enable FlashAttention for faster inference (if using PyTorch 2.0+)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "# âœ… Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    "\n",
    "# âœ… Ensure pad token is set correctly\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# âœ… Use BF16 instead of FP16 (More stable, similar performance)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # âœ… 4-bit quantization (Faster than 8-bit)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # âœ… Use BF16 instead of FP16\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized 4-bit (best performance)\n",
    ")\n",
    "\n",
    "# âœ… Load model with quantization & auto device placement\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # âœ… Use BF16 (better stability)\n",
    "    device_map=\"auto\",  # Automatically allocate layers across GPU & CPU\n",
    "    quantization_config=bnb_config,  # Use BitsAndBytesConfig for quantization\n",
    ").to('cuda')\n",
    "\n",
    "# âœ… Compile model for faster execution (PyTorch 2.0+)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d748f922-354f-4261-9f85-db3baa91f921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 4096  # Number of tokens to generate\n",
    "\n",
    "# âœ… Prepare input and move it to GPU\n",
    "prompt = \"hello\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# âœ… Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# âœ… Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe235fa-1872-4200-b1de-7cadd17d4bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain to me about reinforcement learning\n",
      "\n",
      "Okay, so I'm trying to understand reinforcement learning. I've heard about it in the context of AI and machine learning, but I'm not entirely sure how it works. Let me start by breaking down the term. \"Reinforcement\" suggests something that strengthens, and \"learning\" is about acquiring knowledge. So, reinforcement learning must be a method where an agent learns by strengthening certain behaviors.\n",
      "\n",
      "From what I remember, reinforcement learning is different from supervised learning because it doesn't have a labeled dataset. Instead, the agent interacts with an environment and learns through trial and error. The agent takes actions, observes the outcomes, and adjusts its behavior based on the feedback. That feedback is usually in the form of rewards or penalties, which guide the learning process.\n",
      "\n",
      "I think the key components are the agent, the environment, actions, states, and rewards. The agent is the learner or decision-maker, the environment is the world it interacts with. The agent takes actions, which are the steps it takes, and the environment is in a state, which is the current condition. Each action leads to a new state and a reward, which tells the agent how good or bad that action was.\n",
      "\n",
      "So, the goal is for the agent to learn a policy, which is a strategy that maps states to actions to maximize the cumulative reward over time. That makes sense. The policy could be deterministic, where it always chooses the same action for a state, or stochastic, where it chooses actions probabilistically.\n",
      "\n",
      "I've also heard about value functions, which estimate the expected return (total reward) starting from a state or state-action pair. There are Q-values, which are the expected returns for taking a specific action in a state, and V-values, which are the expected returns for a state under the current policy.\n",
      "\n",
      "Exploration vs. exploitation is another concept. The agent needs to explore the environment to discover high-reward actions but also exploit what it already knows to maximize rewards. Balancing these is crucial because too much exploration might delay high rewards, while too much exploitation might miss better options.\n",
      "\n",
      "I'm a bit confused about the difference between Q-learning and policy gradient methods. Q-learning focuses on learning the Q-values and then choosing the best action, whereas policy gradient methods directly optimize the policy by adjusting the parameters to maximize the expected reward. I think Q-learning is more about value-based learning, while policy gradient is about policy-based learning.\n",
      "\n",
      "Deep reinforcement learning comes into play when the state space is large, like in complex environments such as video games. Using neural networks allows the agent to handle high-dimensional inputs, like images, and approximate the Q-values or policies, which would be intractable otherwise.\n",
      "\n",
      "Applications of reinforcement learning include game playing, robotics, resource allocation, and autonomous systems. It's used in AlphaGo, which beat top human players, and in training robots to perform tasks in dynamic environments.\n",
      "\n",
      "Challenges include the difficulty in balancing exploration and exploitation, the need for efficient computation due to high-dimensional data, and the problem of delayed rewards where the impact of actions isn't immediate, making it hard to learn cause-and-effect relationships.\n",
      "\n",
      "I'm trying to piece together how the process works step by step. The agent starts in an initial state, takes an action, observes the new state and the reward, and then updates its Q-value or policy based on that. Over time, it accumulates knowledge to make better decisions.\n",
      "\n",
      "I'm still a bit fuzzy on how exactly the learning is updated. I think there's a formula for updating Q-values, like Q(s, a) = Q(s, a) + Î±(r + Î³ Q(s', a') - Q(s, a)), where Î± is the learning rate, Î³ is the discount factor, and r is the reward. This is the Q-learning update rule, right?\n",
      "\n",
      "I also wonder how policies are updated in policy gradient methods. I believe it's through gradient ascent, where the parameters of the policy are adjusted in the direction that increases the expected reward. The policy is a function that outputs probabilities of actions given a state, and the gradient of the reward with respect to the policy parameters is computed and used to update the policy.\n",
      "\n",
      "I'm curious about the difference between on-policy and off-policy methods. On-policy methods learn the policy that is being followed, like in policy gradient and SARSA, while off-policy methods learn a Q-function that may not be the current policy, like Q-learning and Deep Q-Networks.\n",
      "\n",
      "I should also consider practical examples. For instance, in a simple game like a grid where the agent moves right or left, each action gives a reward, and the goal is to reach the end. The agent would start exploring, sometimes moving in directions that give lower rewards, but over time, the Q-values for better actions would increase, guiding the agent to take those actions more frequently.\n",
      "\n",
      "Another example is training a robot to walk. The robot's sensors provide the state, actions are the torques applied to the joints, and rewards could be based on how far it walks without falling. The agent learns the best actions to take in each state to maximize the reward.\n",
      "\n",
      "I'm still not entirely clear on how the discount factor Î³ affects the learning. I think it determines how much the agent values future rewards versus immediate ones. A higher Î³ makes the agent consider the long-term consequences more, leading to more global optimization but potentially making learning slower because the agent is looking further ahead.\n",
      "\n",
      "I also need to understand the difference between model-based and model-free methods. Model-based methods build a model of the environment to plan actions, while model-free methods learn the value functions or policies directly without building an explicit model. Model-based can be more efficient if the model is accurate but requires knowing or learning the dynamics of the environment.\n",
      "\n",
      "I'm a bit confused about how exactly the agent decides when to explore and when to exploit. Techniques like epsilon-greedy, where the agent sometimes takes random actions to explore, and sometimes takes the best known action to exploit, are common. There are also more sophisticated methods like UCB1 in multi-armed bandits, which balance exploration and exploitation based on confidence intervals.\n",
      "\n",
      "I should also think about the limitations of reinforcement learning. One major issue is the sample efficiency, meaning that the agent might need a lot of interactions with the environment to learn, which can be time-consuming and computationally expensive. Another is the problem of sparse rewards, where the agent might not receive feedback often, making it hard to learn which actions are good.\n",
      "\n",
      "I'm trying to grasp the concept of the Bellman equation, which is fundamental in reinforcement learning. It's a way to express the expected return in terms of the current reward and the future rewards. The Bellman equation helps in breaking down the problem into smaller subproblems, making it easier to solve.\n",
      "\n",
      "In summary, reinforcement learning is a method where an agent learns to make decisions by performing actions in an environment, receiving rewards or penalties, and adjusting its behavior to maximize cumulative rewards. It involves key components like states, actions, rewards, policies, and value functions. The agent balances exploration and exploitation to learn the optimal policy, and different methods like Q-learning and policy gradient are used depending on the problem's nature. Challenges include balancing exploration, handling high-dimensional data, and dealing with delayed rewards.\n",
      "\n",
      "I think I have a basic understanding now, but I need to solidify it by looking into specific algorithms, their workings, and perhaps try a simple implementation to see how it all comes together.\n",
      "</think>\n",
      "\n",
      "Reinforcement learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent aims to maximize cumulative rewards by taking actions that lead to favorable outcomes. Here's a structured overview of reinforcement learning:\n",
      "\n",
      "### Key Components:\n",
      "1. **Agent**: The learner that interacts with the environment.\n",
      "2. **Environment**: The world in which the agent operates, providing states and rewards.\n",
      "3. **Actions**: The steps the agent takes to influence the environment.\n",
      "4. **States**: The current condition of the environment.\n",
      "5. **Rewards**: Feedback indicating the quality of the agent's actions.\n",
      "\n",
      "### Learning Process:\n",
      "- The agent starts in an initial state, takes an action, observes the new state and reward, and updates its knowledge (policies or value functions) to make better decisions over time.\n",
      "\n",
      "### Policy and Value Functions:\n",
      "- **Policy (Ï€)**: Maps states to actions, guiding the agent's behavior. It can be deterministic or stochastic.\n",
      "- **Value Functions**: Estimate the expected return from a state or state-action pair. Q-values (Q(s, a)) and V-values (V(s)) are key here.\n",
      "\n",
      "### Exploration vs. Exploitation:\n",
      "- **Exploration**: Discovering new actions and states to find high-reward opportunities.\n",
      "- **Exploitation**: Using known information to maximize rewards. Balancing these is crucial for effective learning.\n",
      "\n",
      "### Learning Methods:\n",
      "1. **Q-Learning**: Learns Q-values, which are the expected returns for state-action pairs. Uses the update rule: \n",
      "   \\[\n",
      "   Q(s, a) = Q(s, a) + \\alpha(r + \\gamma Q(s', a') - Q(s, a))\n",
      "   \\]\n",
      "   where Î± is the learning rate, Î³ is the discount factor, and r is the reward.\n",
      "\n",
      "2. **Policy Gradient Methods**: Directly optimize the policy by adjusting parameters to maximize expected rewards. Uses gradient ascent.\n",
      "\n",
      "3. **Deep RL**: Uses neural networks to handle high-dimensional inputs, approximating Q-values or policies.\n",
      "\n",
      "### Challenges:\n",
      "- **Balancing Exploration/Exploitation**: Techniques like epsilon-greedy balance these.\n",
      "- **Efficiency**: High-dimensional data and delayed rewards can slow learning.\n",
      "- **Sparse Rewards**: Infrequent feedback complicates learning.\n",
      "\n",
      "### Applications:\n",
      "- Game playing (e.g., AlphaGo), robotics, resource allocation, autonomous systems.\n",
      "\n",
      "### Concepts:\n",
      "- **Bellman Equation**: Decomposes the problem into smaller parts, aiding in solving complex tasks.\n",
      "- **Model-Based vs. Model-Free**: Model-based uses environment models for planning; model-free learns directly from interactions.\n",
      "\n",
      "### Conclusion:\n",
      "Reinforcement learning is a powerful approach for decision-making problems, balancing exploration and exploitation to learn optimal policies. It offers flexibility across various applications but faces challenges in efficiency and complexity. Understanding specific algorithms and their mechanics is essential for effective implementation.\n"
     ]
    }
   ],
   "source": [
    "# âœ… Prepare input and move it to GPU\n",
    "prompt = \"explain to me about reinforcement learning\"\n",
    "\n",
    "max_new_tokens = 4096  # Number of tokens to generate\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# âœ… Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# âœ… Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542a4640-e2d8-493e-a00c-4af095b36523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7a1a4-7474-4eb6-8272-5d7f39f969b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66870e21-f46f-4d81-b8a3-c76b3fb02cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700fcdb-9c7f-464f-b9b3-e0d2efed291b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbad9f2-f533-4abe-9d2c-a3c835f1b352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fa011-a1a6-432d-8ffa-01e0766cb552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20aa0a4-9d39-4a75-b6e7-d2a5d0620c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Prepare input and move it to GPU\n",
    "prompt = \"what are you good at?\"\n",
    "\n",
    "max_new_tokens = 4096  # Number of tokens to generate\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# âœ… Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# âœ… Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382b3ce-0947-4e0b-bd3a-4f0c3b3e3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Prepare input and move it to GPU\n",
    "prompt = \"are you good at coding?\"\n",
    "\n",
    "max_new_tokens = 4096  # Number of tokens to generate\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# âœ… Optimize model generation with FlashAttention & BF16\n",
    "generated_output = model.generate(\n",
    "    **inputs, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True,  # Sampling instead of greedy search\n",
    "    temperature=0.7,  # More diverse responses\n",
    "    top_k=50,  # Limits sampling to top 50 words\n",
    "    top_p=0.9,  # Nucleus sampling\n",
    ")\n",
    "\n",
    "# âœ… Decode and print response\n",
    "response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddff1b8-384a-458c-bf5e-f52188892594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba6614-9427-474b-a97a-e877484e9df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1846879-deb0-46b9-bc03-527b488347ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ed523-663d-4d85-91f8-4ee1de56186c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27da2e4-e361-49a4-a87e-64fbc5b3edf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e049-5c0b-49ef-90c0-efa060ae4fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10395677,
     "sourceId": 86411,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.048125,
   "end_time": "2024-12-26T20:02:43.573801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T20:00:20.525676",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
